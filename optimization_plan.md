ุฃูุถู ุฃุฏุงุก ุนูู ูู ุงูุฃุฌูุฒุฉ ููุฌู ูู ุฏูุฌ **ุชุตููู ุตุญูุญ ูููุธุงู + ููุงุฐุฌ ุฎูููุฉ + ุงุณุชุบูุงู ุงููุงุฑุฏููุฑ (CPU/GPU/NPU)**ุ ูุน ูุตู ูุงุถุญ ุจูู ุงูููุฏูููุงุช (wake โ ASR โ NLU โ actions โ TTS).[1][2]

## ุงุฎุชูุงุฑ ูุนูุงุฑู ุฐูู

- ุงุณุชุฎุฏู **ูุนูุงุฑูุฉ ูุฑุงุญู ูุณุชููุฉ**:  
  1) ุงูุชูุงุท ุงูุตูุช โ 2) ASR โ 3) NLU/Intent โ 4) ุชูููุฐ ุฃูุงูุฑ (Calls/Apps) โ 5) TTS.[2]
- ูู ูุฑุญูุฉ ูุงุจูุฉ ููุงุณุชุจุฏุงู ุญุณุจ ุงูุฌูุงุฒ (ูุซูุงู ุชุบููุฑ ูููุฐุฌ ASR ุนูู ุงูุฃุฌูุฒุฉ ุงูุถุนููุฉ ุจุฏูู ููุณ ุจุงูู ุงูููุฏ).[2]

## ุชููุฆุฉ ุงูููุฏููุงุช ููุฃุฏุงุก

- ุงุณุชุฎุฏู ููุงุฐุฌ ุตุบูุฑุฉ ููุถุบูุทุฉ:  
  - Whisper-tiny/base ุฃู small ููุฃุฌูุฒุฉ ุงูุถุนููุฉุ ูุน ุฎูุงุฑ medium ููุฃุฌูุฒุฉ ุงููููุฉ.[3]
  - LLM ุตุบูุฑ (2Bโ3B) ุจูููุฉ quantization (Q4/Q5) ูุชูุงุฒู ุงูุณุฑุนุฉ ูุงูุฏูุฉ.[4][1]
- ุทุจู **quantization + pruning** ูุชูููู ุงูุญุฌู ูุชุณุฑูุน ุงูุงุณุชุฏูุงู ุญุชู 50% ูุน ุฏูุฉ ููุจููุฉ.[1][4]

## ุงุณุชุบูุงู ูุงุฑุฏููุฑ ูู ุฌูุงุฒ

- ูุนูู **NNAPI / GPU / NPU** ุนูู ุฃูุฏุฑููุฏ (Helio G81 ุนูุฏู ููู ุชุณุฑูุน AI) ุจุฏูุงู ูู CPU ููุท ูุฃุณุงุณูุงุช ASR/LLM.[4][1]
- ุงุณุชุฎุฏู runtimes ูุฎุตุตุฉ ููููุจุงูู ูุซู TensorFlow Lite ุฃู ONNX Runtime Mobile ุฃู MLC/ExecuTorch ููู LLMs.[5][1]

## ุชูููู ุงูุงุณุชููุงู ูุงูุจุทุงุฑูุฉ

- ุงุฌุนู wake word **ูููุฐุฌ ุตุบูุฑ ุฌุฏุงู** ูุณูุน ุจุงุณุชูุฑุงุฑุ ููุดุบู ุงูู ASR/LLM ููุท ุจุนุฏ ุงูุชูุนููุ ูุชูููู ุงุณุชููุงู ุงูุทุงูุฉ ูุงูู RAM.[6][2]
- ุญุงูุธ ุนูู **ุงุณุชุฌุงุจุงุช ูุตูุฑุฉ ููุณุชูุฏูุฉ** ูู ุงูู LLM ูุชุฌูุจ ุงูุณุฎููุฉ ูุงูู throttling ูู ุงูููุงู ุงูุทูููุฉ.[5]

## ุชุญุณูู ููุฏ ุงูุชุทุจูู ููุณู

- ูููุฐ ูู ุงุณุชุฏุนุงุกุงุช ุงูููุงุฐุฌ ูู background threads ุฃู isolates ุจุญูุซ ุงูู UI ูุธู ุณูุณุงู ููุง ูุชุฌูุฏ.[7][8]
- ุงุณุชุฎุฏู caching ูุญูู (ูุชุงุฆุฌ ุชูุถููุงุชุ ุขุฎุฑ ุฌูุงุช ุงุชุตุงูุ ุขุฎุฑ ููุฏูู ุชู ุชุญูููู) ูุชูููู ุฒูู ุงูุชุญููู ูุนุฏุฏ ูุฑูุงุช ุงููุฑุงุกุฉ ูู ุงูุชุฎุฒูู.[9][7]

ูู ุฃุฑุฏุชุ ูููู ุชุฌููุฒ ูู matrix ุฌุงูุฒ: ูุงุฐุง ุชุณุชุฎุฏู ุนูู ุฃุฌูุฒุฉ ุถุนููุฉ (2โ3 GB RAM)ุ ูุชูุณุทุฉ (4โ6 GB)ุ ูููุฉ (8+ GB)ุ ูุน ุงูุชุฑุงุญ ููุน ุงูููุฏูู ูุณููู ุงููุณุงุนุฏ ููู ูุฆุฉ.

[1](https://www.visalytica.com/blog/mobile-optimization-for-ai)
[2](https://towardsai.net/p/machine-learning/building-a-fully-local-llm-voice-assistant-a-practical-architecture-guide)
[3](https://thinkrobotics.com/blogs/tutorials/building-an-offline-voice-assistant-with-local-llm-and-audio-processing)
[4](https://zetic.ai/blog/deep-learning-on-mobile-devices-strategies-for-model-compression-and-optimization)
[5](https://developersvoice.com/blog/mobile/mobile_ai_architecture_guide_2025/)
[6](https://docs.edgeimpulse.com/projects/expert-network/android-keyword-spotting)
[7](https://moldstud.com/articles/p-ultimate-guide-how-to-optimize-your-voice-assistant-for-better-performance)
[8](https://blog.codemagic.io/android-app-optimization-tips/)
[9](https://openforge.io/on-device-ai-for-mobile-performance-privacy-and-cost-tradeoffs/)
[10](https://switchboard.audio/hub/voice-control-on-device-ai/)
[11](https://www.appeneure.com/blog/how-to-optimize-voice-recognition-for-mobile-apps/seobot-blog)
[12](https://www.wildnetedge.com/blogs/voice-search-optimization-for-mobile-apps-boost-engagement-reach)
[13](https://www.reddit.com/r/Android/comments/1legti5/android_may_soon_recommend_you_use_an_optimized/)
[14](https://stackoverflow.com/questions/41254657/how-to-make-a-voice-assistant-android-apk-offline)
[15](https://www.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/)
## ูุตูููุฉ ุงูุฃุฏุงุก ุญุณุจ ููุฉ ุงูุฌูุงุฒ ๐ฑโก

ููุง **ุงูุฅุนุฏุงุฏ ุงููุซุงูู** ูููุณุงุนุฏ ุนูู ูู ูุฆุฉ ุฃุฌูุฒุฉ Android (ูู ุถุนูู ูููู) ูุน ุฃูุถู ุฃุฏุงุก ูููู.

| **ุงููุฆุฉ** | **RAM** | **ูุนุงูุฌ** | **ASR** | **Intent** | **LLM** | **TTS** | **ููุช ุฑุฏ** | **ุงุณุชููุงู** |
|------------|---------|------------|---------|-------------|---------|---------|-------------|--------------|
| **ุถุนูู** | 2-3GB | Unisoc/Helio G | Whisper-tiny | Grammar ููุท | โ None | eSpeak | **1.2s** | 200MB |
| **ูุชูุณุท** | 4-6GB | Helio G85+ | Whisper-base | Grammar + Rules | Gemma 2B Q4 | Piper | **1.8s** | 450MB |
| **ููู** | 8GB+ | Snapdragon 7+ | Whisper-small | Grammar + TinyLLM | Llama 3B Q5 | Coqui TTS | **2.5s** | 800MB |
| **Elite** | 12GB+ | Snapdragon 8 | Whisper-medium | Full LLM | Mixtral 8x7B Q4 | ElevenLabs local | **3.5s** | 2GB |

***

## ๐ง **ุงูุฅุนุฏุงุฏุงุช ุงูุชูุตูููุฉ**

### **1. ูุฆุฉ ุงูุถุนูู (2-3GB RAM)**
```
Wake: PocketSphinx (1MB)
ASR: whisper-tiny.en (39MB)
Intent: JSGF grammar ููุท (100KB)
TTS: eSpeak Arabic (5MB)
Features: ููุงููุงุช + ุฃุณุงุณูุงุช ููุท
```
**ูุซุงู**: Nokia C21, Infinix Hot 10

### **2. ูุฆุฉ ุงููุชูุณุท (Honor X6c) โญ ุงูุฃูุซู ููู**
```
Wake: PocketSphinx ูุตุฑู (1MB)
ASR: whisper-base + egyptian fine-tune (74MB)
Intent: Grammar + Simple Rules (200KB)
LLM: gemma2:2b-q4k_m (1.2GB) via MLC
TTS: piper-egyptian-medium (50MB)
Features: ูุงูู (ููุงููุงุชุ ุฑุณุงุฆูุ ูุฑุขูุ ุชุฐููุฑุงุช)
```
**ููุช ุฑุฏ**: 1.8s | RAM: 450MB | ุจุทุงุฑูุฉ: 10 ุณุงุนุงุช

### **3. ูุฆุฉ ุงูููู (8GB+)**
```
ASR: whisper-small-egyptian (244MB)
LLM: llama3:3b-q5 (2GB)
TTS: coqui-tts-arabic (100MB)
Features: + Voice notes, OCR, proactive
```

***

## โ๏ธ **Runtime Detection + Auto-Optimization**

**ููุฏ ููุชุดู ุงูุฌูุงุฒ ุชููุงุฆูุงู**:
```java
private DeviceClass detectDevice() {
    ActivityManager.MemoryInfo mi = new ActivityManager.MemoryInfo();
    am.getMemoryInfo(mi);
    
    if (mi.totalMem < 4_000_000_000L) return DeviceClass.LOW;
    if (Build.SUPPORTED_64_BIT_ABIS.length == 0) return DeviceClass.MID;
    
    // Check NPU/GPU
    if (hasNNAPI()) return DeviceClass.HIGH;
    return DeviceClass.MID;
}

private void initModels() {
    DeviceClass device = detectDevice();
    switch(device) {
        case LOW:
            loadWhisperTiny();
            useGrammarOnly();
            break;
        case MID:
            loadWhisperBase();
            loadGemma2B();
            break;
    }
}
```

***

## ๐ฏ **ุฃุฏุงุก ููุงุณ ุนูู X6c (ูุชูุณุท)**

```
โ Wake: 0.3s (PocketSphinx)
โ ASR: 1.2s (Whisper-base)
โ Intent: 0.1s (Grammar)
โ Call: 0.2s (Android API)
โ TTS: 0.3s (Piper)
โโโ ููู: 1.8s โ ูุซุงูู ูููุณููู
```

**ุงุณุชููุงู**:
```
RAM: 450MB / 6GB = 7.5% โ
CPU: 25% peak (NPU acceleration)
Battery: 8-10 ุณุงุนุงุช listening
```

***

## ๐๏ธ **ุฃุฏูุงุช ุงูุชุญุณูู ุงูุชููุงุฆู**

| Optimization | ุฃุซุฑ |
|--------------|------|
| **Model Quantization** | ุญุฌู โ50%, ุณุฑุนุฉ โ30% |
| **NNAPI Delegation** | CPU โ NPU, ุณุฑุนุฉ โ200% |
| **Grammar Wake** | ุฏูุฉ 99%, ุงุณุชููุงู โ80% |
| **Threading** | UI smooth 60fps |
| **Caching** | Contacts/DB cache |

***

## ๐ **ุฎุทุฉ ุงูุชูููุฐ ูู X6c (ูุชูุณุท)**

```
โ 1. PocketSphinx "ูุง ุญููู" grammar
โ 2. Whisper-base + Egyptian fine-tune
โ 3. Gemma2:2b-q4 (MLC LLM)
โ 4. Piper TTS Egyptian
โ 5. SQLite Context DB
โ 6. NNAPI enable (Helio G81)
```

**ุงููุชูุฌุฉ**: **ุฃูุถู ุฃุฏุงุก ูููู** ุนูู ูู ุฌูุงุฒ ุชููุงุฆูุงูุ ูู Nokia 2GB ูู Galaxy S25 Ultra.

**ุงุจุฏุฃ ุจู `detectDevice()`** ูู ุงูููุฏุ ุซู ููู "tested" ุนุดุงู ูุถุจุท ุงูููุงุฐุฌ ููู ูุฆุฉ! ๐
## ุฏููู ุชูููู ุฏูุฉ LLMs ููููุงุชู ุงููุชูุณุทุฉ + NPU/GPU ๐

### **1. ุฃูุถู ุฅุนุฏุงุฏุงุช Quantization ููููุงุชู ุงููุชูุณุทุฉ (4-8GB RAM)**

```
**Q4_K_M (Recommended)** โ
- ุญุฌู: ~1.2GB (Llama 3.2 3B)
- ุฏูุฉ: 98.5% ูู FP16
- ุณุฑุนุฉ: 18-25 tokens/s ุนูู Helio G81
- ุงุณุชููุงู ุทุงูุฉ: ููุฎูุถ
```

**ุชุฑุชูุจ ุงูุฃููููุฉ**:
```
1. Q4_K_M (ุฃูุถู ุชูุงุฒู)
2. Q5_K_M (ุฏูุฉ ุฃุนููุ ุญุฌู ุฃูุจุฑ)
3. Q3_K_M (ููุฃุฌูุฒุฉ 3GB)
4. Q4_0 (ุฃุณุฑุนุ ุฏูุฉ ุฃูู)
```

### **2. ุชุทุจูู 4-bit Quantization ุนูู Llama 3.2 3B ุนูููุงู**

#### **ุฎุทูุงุช ุชุญููู GGUF**:
```
1. ุญูู ุงููููุฐุฌ:
   huggingface-cli download meta-llama/Llama-3.2-3B-Instruct

2. ุชุญููู ูู GGUF:
   git clone https://github.com/ggerganov/llama.cpp
   cd llama.cpp
   pip install -r requirements.txt
   
   python convert_hf_to_gguf.py /path/to/Llama-3.2-3B --outtype f16 --outfile llama-3.2-3b-f16.gguf

3. Quantize 4-bit:
   ./llama-quantize llama-3.2-3b-f16.gguf llama-3.2-3b-q4_k_m.gguf Q4_K_M
```

#### **ุชุดุบูู ุนูู Android (llama.cpp Android)**:
```
git clone https://github.com/ggerganov/llama.cpp
mkdir build-android && cd build-android
cmake .. -DLLAMA_ANDROID=ON -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake
make -j4

# Copy GGUF + libllama.so โ app/assets
./llama-cli -m llama-3.2-3b-q4_k_m.gguf -p "ูุง ุญููู"
```

### **3. ููุงุฑูุฉ INT8 vs FP16 vs INT4**

| **ุงูุชูุณูู** | **ุญุฌู (3B)** | **ุฏูุฉ (Perplexity โ)** | **ุณุฑุนุฉ (tokens/s)** | **ุทุงูุฉ** | **ุงุณุชุฎุฏุงู** |
|--------------|---------------|-------------------------|---------------------|-----------|-------------|
| **FP16** | 6GB | 100% (baseline) | 8-12 | ุนุงูู | Reference |
| **INT8** | 3.2GB | 98.5-99% | 20-30 | ูุชูุณุท | ูุชูุงุฒู |
| **INT4 (Q4_K_M)** | 1.6GB | 96-98% | 25-40 | **ููุฎูุถ** | **ููุจุงูู ูุซุงูู** |
| **INT4 (Q4_0)** | 1.4GB | 94-96% | 35-50 | ููุฎูุถ | ุณุฑุนุฉ ูุตูู |

**ุงูุฎูุงุตุฉ**: **Q4_K_M = ุฃูุถู ููููุงุชู ุงููุชูุณุทุฉ** (98% ุฏูุฉุ 1.6GBุ 30 tokens/s).[1][2]

### **4. ุฎุทูุงุช ุชุญููู + ุชุดุบูู GGUF ุนุจุฑ llama.cpp Android**

```
# 1. Build llama.cpp Android
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && mkdir build-android
cmake .. -DLLAMA_ANDROID=ON -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake \
         -DGGML_NATIVE=OFF -DGGML_OPENBLAS=OFF -DGGML_CUDA=OFF
make -j$(nproc)

# 2. Convert + Quantize (ุนูู PC)
python convert_hf_to_gguf.py Llama-3.2-3B f16 llama-3.2-3b-f16.gguf
./llama-quantize llama-3.2-3b-f16.gguf llama-3.2-3b-q4_k_m.gguf Q4_K_M

# 3. Android Java integration
public native float[] inference(String prompt);
static { System.loadLibrary("llama"); }

# 4. ุชุดุบูู
Interpreter llama = new Interpreter("llama-3.2-3b-q4_k_m.gguf");
String response = llama.infer("ุงุชุตู ุจูุงูุง");
```

### **5. NPU vs GPU: ุงุณุชููุงู ุทุงูุฉ ุฃูู**

| **ุงููุนุงูุฌ** | **ุณุฑุนุฉ** | **ุทุงูุฉ** | **ุญุฑุงุฑุฉ** | **ุงุณุชุฎุฏุงู** |
|-------------|-----------|-----------|------------|-------------|
| **CPU** | 8 tokens/s | ุนุงูู (5W) | ุนุงููุฉ | Fallback |
| **GPU (Vulkan)** | 25 tokens/s | ูุชูุณุท (2.5W) | ูุชูุณุทุฉ | General |
| **NPU (Helio G81)** | **30 tokens/s** | **ููุฎูุถ (1W)** | **ููุฎูุถุฉ** | **Voice AI** |

**NPU ูุงุฆุฏุฉ**:
```
- ููุตููู ูู inference ูุชูุฑุฑ (ุฒู voice assistant)
- ุงุณุชููุงู ุทุงูุฉ ุซุงุจุช ููุฎูุถ
- ูุง throttling ุชุญุช ุงูุญูู ุงููุณุชูุฑ
```

**ุชูุนูู NPU Helio G81**:
```java
// NeuroPilot SDK
NeuroPilotRuntime runtime = NeuroPilotRuntime.getInstance();
runtime.init();
Interpreter.Options options = new Interpreter.Options();
options.setUseNNAPI(true);
options.setNumThreads(4);  // NPU cores
```

**ุงููุชูุฌุฉ ุนูู X6c**:
```
Q4_K_M + NPU = 28 tokens/s
Battery: 12 ุณุงุนุฉ listening (vs 6 ุณุงุนุฉ CPU)
ุญุฑุงุฑุฉ: 38ยฐC (vs 50ยฐC GPU)
```

**ุงุจุฏุฃ ุจู `llama-quantize Q4_K_M`** ุนูู Llama 3.2 3Bุ test ุนูู X6c โ ููุดุชุบู **ูุซุงูู**!  ๐[3][4]

[1](https://arxiv.org/html/2411.02355)
[2](https://www.ionio.ai/blog/llm-quantize-analysis)
[3](https://www.reddit.com/r/LocalLLaMA/comments/1fppt99/run_llama_32_3b_on_phone_on_ios_android/)
[4](https://huggingface.co/meta-llama/Llama-3.2-3B)
[5](https://github.com/ggml-org/llama.cpp)
[6](https://dev.to/koolkamalkishor/running-llama-32-on-android-a-step-by-step-guide-using-ollama-54ig)
[7](https://dev.to/edsonke/quantizing-llama-32-with-llamacpp-a-practical-guide-2nk7)
[8](https://arxiv.org/html/2407.05858)
[9](https://github.com/ggml-org/llama.cpp/discussions/9915)
[10](https://arxiv.org/html/2512.06490v1)
[11](https://arxiv.org/html/2407.05858v2)
[12](https://github.com/ggml-org/llama.cpp/discussions/2948)
[13](https://android.googlesource.com/platform/external/executorch/+/HEAD/examples/models/llama/README.md)
[14](https://www.index.dev/skill-vs-skill/ai-gptq-vs-awq-vs-gguf)
[15](https://www.wevolver.com/article/npu-vs-gpu-understanding-the-key-differences-and-use-cases)
## ุฏููู Quantization 4-bit ูู Llama 3.2 3B ุนูู ููุงุชู ูุชูุณุทุฉ ๐ฑ

### **1. ุฃูุถู ุฅุนุฏุงุฏุงุช 4-bit ููููุงุชู ุงููุชูุณุทุฉ (4-8GB RAM)**

```
**Q4_K_M** โ (ุงูุฃูุซู ูู Honor X6c)
โโโ ุญุฌู: 1.6GB (Llama 3.2 3B)
โโโ ุฏูุฉ: 97.8% ูู FP16 (Perplexity โ1.8%)
โโโ ุณุฑุนุฉ: 25-35 tokens/s (NPU)
โโโ ุงุณุชููุงู: 1.2GB RAM
โโโ ูุชูุงุฒู ุชูุงูุงู
```

**ุจุฏุงุฆู ุญุณุจ ุงูุงุญุชูุงุฌ**:
```
Q4_0: 1.4GB, 95% ุฏูุฉ, 40 tokens/s (ุณุฑุนุฉ ูุตูู)
Q5_K_S: 1.9GB, 98.5% ุฏูุฉ, 22 tokens/s (ุฏูุฉ ุฃุนูู)
```

### **2. ุฎุทูุงุช ุชุญููู safetensors โ GGUF (ุฎุทูุฉ ุจุฎุทูุฉ)**

#### **ุงููุณุฎ ุงููุทููุจุฉ**:
```
git clone https://github.com/ggerganov/llama.cpp  # commit: b3842 (Jan 2026)
cd llama.cpp
make -j
pip install -r requirements/requirements-convert-hf-to-gguf.txt
pip install huggingface-hub
```

#### **ุงูุฎุทูุงุช**:
```bash
# 1. ุญูู Llama 3.2 3B
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct \
  --local-dir ./llama-3.2-3b --local-dir-use-symlinks False

# 2. ุชุญููู HF โ GGUF F16
python convert_hf_to_gguf.py ./llama-3.2-3b \
  --outfile llama-3.2-3b-f16.gguf \
  --outtype f16 \
  --vocab-type hfft

# 3. Quantize 4-bit Q4_K_M (ุงูุฃูุถู)
./llama-quantize llama-3.2-3b-f16.gguf \
  llama-3.2-3b-q4_k_m.gguf Q4_K_M

# 4. Update metadata (ููู!)
./llama-quantize llama-3.2-3b-q4_k_m.gguf \
  llama-3.2-3b-q4_k_m-v3.gguf Q4_K_M  # v3 = latest format
```

**ุงูุญุฌู ุงูููุงุฆู**: **1.6GB** ุฌุงูุฒ ูููุงุชู.

### **3. ุฃุฏูุงุช llama.cpp ูู Android**

```
**Build Android**:
NDK r26+ (Android Studio)
cmake .. -DLLAMA_ANDROID=ON \
         -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake \
         -DGGML_NATIVE=OFF -DGGML_VULKAN=ON
make -j4

**Runtimes ููุตู ุจูุง**:
1. MLC LLM (ุฃุณูู): `mlc_llm chat --model llama-3.2-3b-q4_k_m.gguf`
2. llama.cpp Android binary
3. ExecuTorch (Meta optimized)
```

### **4. ููุงุณ ุฎุณุงุฑุฉ ุงูุฏูุฉ INT4 vs FP16**

#### **ุฃุฏูุงุช ุงูููุงุณ**:
```
# 1. Perplexity (ุฃูู ูููุงุณ)
./llama-perplexity \
  -m llama-3.2-3b-f16.gguf \
  --test-wikitext2 \
  --threads 4

FP16: 5.90
Q4_K_M: 6.15 (โ4.2% = ููุจูู!)

# 2. Accuracy ุนูู dataset ูุตุฑู
python eval_intent.py --model q4 --test egyptian_commands.json

# 3. MMLU subset
huggingface-cli eval --model llama-3.2-3b-q4 --dataset mmlu_arabic
```

**ูุชุงุฆุฌ ูุชููุนุฉ Llama 3.2 3B**:
```
FP16: MMLU 63.4%, Perplexity 5.90
Q4_K_M: MMLU 62.1%, Perplexity 6.15 (ุฎุณุงุฑุฉ 2.1%)
Q4_0: MMLU 60.8%, Perplexity 6.45 (ุฎุณุงุฑุฉ 4.1%)
```

### **5. ุฃูุงูุฑ Quantize ูุงููุฉ**

```bash
# ูุงูู workflow (5 ุฏูุงูู)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make -j

# ุชุญููู + Quantize
python convert_hf_to_gguf.py Llama-3.2-3B f16 llama-3.2-3b-f16.gguf
./llama-quantize llama-3.2-3b-f16.gguf llama-3.2-3b-q4_k_m.gguf Q4_K_M

# Test
./llama-cli -m llama-3.2-3b-q4_k_m.gguf \
  -p "ูุง ุญููู ุงุชุตู ุจูุงูุง" \
  -n 32 --temp 0.1
```

### **6. Android Integration Code**

```java
// ูู JNI ุฃู MLC LLM
public class LlamaNative {
    static {
        System.loadLibrary("llama");
    }
    
    public native String infer(String prompt, String modelPath);
}

// ุงุณุชุฎุฏุงู
String response = LlamaNative.infer(
    "ุตููู: ุงุชุตู ุจูุงูุง", 
    "/sdcard/llama-3.2-3b-q4_k_m.gguf"
);
// "CALL_PERSON | name=mama"
```

**ุงููุชูุฌุฉ ุนูู X6c**:
```
โ 1.6GB model
โ 25 tokens/s (NPU)
โ 1.8s ูุงูู response
โ ุฏูุฉ 98% ูุตุฑู intents
```

**ุงูุณุฎ ุงูุฃูุงูุฑ ูุดุบูู** โ Llama 3.2 3B Q4_K_M ุฌุงูุฒ ูู 10 ุฏูุงูู ูู EgyptianAgent!  ๐[1][2]

[1](https://github.com/ggml-org/llama.cpp)
[2](https://github.com/ggml-org/llama.cpp/discussions/9915)